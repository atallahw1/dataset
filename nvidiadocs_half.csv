Title,Heading,Content
Introduction,Abstract,"This NVIDIA TensorRT Developer Guide demonstrates how to use the C++ and Python APIs for implementing the most common deep learning layers. It shows how you can take an existing model built with a deep learning framework and build a TensorRT engine using the provided parsers. The Developer Guide also provides step-by-step instructions for common user tasks such as creating a TensorRT network definition, invoking the TensorRT builder, serializing and deserializing, and how to feed the engine with data and perform inference; all while using either the C++ or Python API."
Introduction,Introduction,"NVIDIA® TensorRT™ is an SDK that facilitates high-performance machine learning inference. It is designed to work in a complementary fashion with training frameworks such as TensorFlow, PyTorch, and MXNet. It focuses specifically on running an already-trained network quickly and efficiently on NVIDIA hardware."
Introduction,Structure of this guide,"Chapter 1 provides information about how TensorRT is packaged and supported, and how it fits into the developer ecosystem. Chapter 2 provides a broad overview of TensorRT capabilities. Chapters three and four contain introductions to the C++ and Python APIs respectively. Subsequent chapters provide more detail about advanced features."
Introduction,Samples,The NVIDIA TensorRT Sample Support Guide illustrates many of the topics discussed in this guide. Additional samples focusing on embedded applications can be found here.
Introduction,Complementary GPU Features,"Multi-Instance GPU, or MIG, is a feature of NVIDIA GPUs with NVIDIA Ampere Architecture or later architectures that enable user-directed partitioning of a single GPU into multiple smaller GPUs. The physical partitions provide dedicated compute and memory slices with QoS and independent execution of parallel workloads on fractions of the GPU. For TensorRT applications with low GPU utilization, MIG can produce higher throughput at small or no impact on latency. The optimal partitioning scheme is application-specific."
Introduction,Complementary Software,"The NVIDIA Triton™ Inference Server is a higher-level library providing optimized inference across CPUs and GPUs. It provides capabilities for starting and managing multiple models, and REST and gRPC endpoints for serving inference. NVIDIA DALI® provides high-performance primitives for preprocessing image, audio, and video data. TensorRT inference can be integrated as a custom operator in a DALI pipeline. A working example of TensorRT inference integrated as a part of DALI can be found here. TensorFlow-TensorRT (TF-TRT) is an integration of TensorRT directly into TensorFlow. It selects subgraphs of TensorFlow graphs to be accelerated by TensorRT, while leaving the rest of the graph to be executed natively by TensorFlow. The result is still a TensorFlow graph that you can execute as usual. For TF-TRT examples, refer to Examples for TensorRT in TensorFlow. Torch-TensorRT (Torch-TRT) is a PyTorch-TensorRT compiler that converts PyTorch modules into TensorRT engines. Internally, the PyTorch modules are first converted into TorchScript/FX modules based on the Intermediate Representation (IR) selected. The compiler selects subgraphs of the PyTorch graphs to be accelerated by TensorRT, while leaving the rest of the graph to be executed natively by Torch. The result is still a PyTorch module that you can execute as usual. For examples, refer to Examples for Torch-TRT. The TensorFlow-Quantization toolkit provides utilities for training and deploying Tensorflow 2-based Keras models at reduced precision. This toolkit is used to quantize different layers in the graph exclusively based on operator names, class, and pattern matching. The quantized graph can then be converted into ONNX and then into TensorRT engines. For examples, refer to the model zoo. The PyTorch Quantization Toolkit provides facilities for training PyTorch models at reduced precision, which can then be exported for optimization in TensorRT. In addition, the PyTorch Automatic SParsity (ASP) tool provides facilities for training models with structured sparsity, which can then be exported and allows TensorRT to use the faster sparse tactics on NVIDIA Ampere Architecture GPUs. TensorRT is integrated with NVIDIA’s profiling tools, NVIDIA Nsight™ Systems and NVIDIA® Deep Learning Profiler (DLProf). A restricted subset of TensorRT is certified for use in NVIDIA DRIVE® products. Some APIs are marked for use only in NVIDIA DRIVE and are not supported for general use."
Introduction,ONNX,"TensorRT’s primary means of importing a trained model from a framework is through the ONNX interchange format. TensorRT ships with an ONNX parser library to assist in importing models. Where possible, the parser is backward compatible up to opset 7; the ONNX Model Opset Version Converter can assist in resolving incompatibilities. The GitHub version may support later opsets than the version shipped with TensorRT refer to the ONNX-TensorRT operator support matrix for the latest information on the supported opset and operators. The ONNX operator support list for TensorRT can be found here. PyTorch natively supports ONNX export. For TensorFlow, the recommended method is tf2onnx. A good first step after exporting a model to ONNX is to run constant folding using Polygraphy. This can often solve TensorRT conversion issues in the ONNX parser and generally simplify the workflow. For details, refer to this example. In some cases, it may be necessary to modify the ONNX model further, for example, to replace subgraphs with plug-ins or reimplement unsupported operations in terms of other operations. To make this process easier, you can use ONNX-GraphSurgeon."
Introduction,Code Analysis Tools,"For guidance using the valgrind and clang sanitizer tools with TensorRT, refer to the Troubleshooting chapter."
Introduction,API Versioning,"TensorRT version number (MAJOR.MINOR.PATCH) follows Semantic Versioning 2.0.0 for its public APIs and library ABIs. Version numbers change as follows: MAJOR version when making incompatible API or ABI changes. PATCH version when making backward compatible bug fixes. Note that semantic versioning does not extend to serialized objects. To reuse plan files, and timing caches, version numbers must match across major, minor, patch, and build versions (with some exceptions for the safety runtime as detailed in the NVIDIA DRIVE OS 6.0 Developer Guide). Calibration caches can typically be reused within a major version but compatibility is not guaranteed."
Introduction,Deprecation Policy,"Deprecation is used to inform developers that some APIs and tools are no longer recommended for use. Beginning with version 8.0, TensorRT has the following deprecation policy: Deprecation notices are communicated in the TensorRT Release Notes. When using the C++ API: API functions are marked with the TRT_DEPRECATED_API macro. Enums are marked with the TRT_DEPRECATED_ENUM macro. All other locations are marked with the TRT_DEPRECATED macro. Classes, functions, and objects will have a statement documenting when they were deprecated. When using the Python API, deprecated methods and classes will issue deprecation warnings at runtime, if they are used. TensorRT provides a 12-month migration period after the deprecation. APIs and tools continue to work during the migration period. After the migration period ends, APIs and tools are removed in a manner consistent with semantic versioning. For any APIs and tools specifically deprecated in TensorRT 7.x, the 12-month migration period starts from the TensorRT 8.0 GA release date."
Introduction,Hardware Support Lifetime,TensorRT 8.5 will be the last release supporting NVIDIA Kepler (SM 3.x) devices. Support for Maxwell (SM 5.x) devices will be dropped in TensorRT 9.0.
Introduction,Support,"Support, resources, and information about TensorRT can be found online at https://developer.nvidia.com/tensorrt. This includes blogs, samples, and more. In addition, you can access the NVIDIA DevTalk TensorRT forum at https://devtalk.nvidia.com/default/board/304/tensorrt/ for all things related to TensorRT. This forum offers the possibility of finding answers, making connections, and getting involved in discussions with customers, developers, and TensorRT engineers."
Introduction,Reporting Bugs,"NVIDIA appreciates all types of feedback. If you encounter any problems, follow the instructions in the Reporting TensorRT Issues section to report the issues."
TensorRT's Capabilities,C++ and Python APIs,"TensorRT’s API has language bindings for both C++ and Python, with nearly identical capabilities. The Python API facilitates interoperability with Python data processing toolkits and libraries like NumPy and SciPy. The C++ API can be more efficient, and may better meet some compliance requirements, for example in automotive applications. Note: The Python API is not available for all platforms. For more information, refer to the NVIDIA TensorRT Support Matrix."
TensorRT's Capabilities,The Programming Model,"TensorRT operates in two phases. In the first phase, usually performed offline, you provide TensorRT with a model definition, and TensorRT optimizes it for a target GPU. In the second phase, you use the optimized model to run inference."
TensorRT's Capabilities,The Build Phase,"The highest-level interface for the build phase of TensorRT is the Builder (C++, Python). The builder is responsible for optimizing a model, and producing an Engine. In order to build an engine, you must: Create a network definition. Specify a configuration for the builder. Call the builder to create the engine. The NetworkDefinition interface (C++, Python) is used to define the model. The most common path to transfer a model to TensorRT is to export it from a framework in ONNX format, and use TensorRT’s ONNX parser to populate the network definition. However, you can also construct the definition step by step using TensorRT’s Layer (C++, Python) and Tensor (C++, Python) interfaces. Whichever way you choose, you must also define which tensors are the inputs and outputs of the network. Tensors that are not marked as outputs are considered to be transient values that can be optimized away by the builder. Input and output tensors must be named, so that at runtime, TensorRT knows how to bind the input and output buffers to the model. The BuilderConfig interface (C++, Python) is used to specify how TensorRT should optimize the model. Among the configuration options available, you can control TensorRT’s ability to reduce the precision of calculations, control the tradeoff between memory and runtime execution speed, and constrain the choice of CUDA® kernels. Since the builder can take minutes or more to run, you can also control how the builder searches for kernels, and cached search results for use in subsequent runs. After you have a network definition and a builder configuration, you can call the builder to create the engine. The builder eliminates dead computations, folds constants, and reorders and combines operations to run more efficiently on the GPU. It can optionally reduce the precision of floating-point computations, either by simply running them in 16-bit floating point, or by quantizing floating point values so that calculations can be performed using 8-bit integers. It also times multiple implementations of each layer with varying data formats, then computes an optimal schedule to execute the model, minimizing the combined cost of kernel executions and format transforms. The builder creates the engine in a serialized form called a plan, which can be deserialized immediately, or saved to disk for later use. Note: Engines created by TensorRT are specific to both the TensorRT version with which they were created and the GPU on which they were created. TensorRT’s network definition does not deep-copy parameter arrays (such as the weights for a convolution). Therefore, you must not release the memory for those arrays until the build phase is complete. When importing a network using the ONNX parser, the parser owns the weights, so it must not be destroyed until the build phase is complete. The builder times algorithms to determine the fastest. Running the builder in parallel with other GPU work may perturb the timings, resulting in poor optimization."
TensorRT's Capabilities,The Runtime Phase,"The highest-level interface for the execution phase of TensorRT is the Runtime (C++, Python). When using the runtime, you will typically carry out the following steps: Deserialize a plan to create an engine. Create an execution context from the engine. Then, repeatedly: Populate input buffers for inference. Call enqueueV3() on the execution context to run inference. The Engine interface (C++, Python) represents an optimized model. You can query an engine for information about the input and output tensors of the network - the expected dimensions, data type, data format, and so on. The ExecutionContext interface (C++, Python), created from the engine is the main interface for invoking inference. The execution context contains all of the state associated with a particular invocation - thus you can have multiple contexts associated with a single engine, and run them in parallel. When invoking inference, you must set up the input and output buffers in the appropriate locations. Depending on the nature of the data, this may be in either CPU or GPU memory. If not obvious based on your model, you can query the engine to determine in which memory space to provide the buffer. After the buffers are set up, inference can be invoked asynchronously (enqueueV3). The required kernels are enqueued on a CUDA stream, and control is returned to the application as soon as possible. Some networks require multiple control transfers between CPU and GPU, so control may not return immediately. To wait for completion of asynchronous execution, synchronize on the stream using cudaStreamSynchronize."
TensorRT's Capabilities,Plug-Ins,"TensorRT has a Plugin interface to allow applications to provide implementations of operations that TensorRT does not support natively. Plug-ins that are created and registered with TensorRT’s PluginRegistry can be found by the ONNX parser while translating the network. TensorRT ships with a library of plug-ins, and source for many of these and some additional plug-ins can be found here. Refer to the Extending TensorRT with Custom Layers chapter for more details."
TensorRT's Capabilities,Types and Precision,"TensorRT supports computations using FP32, FP16, INT8, Bool, and INT32 data types. When TensorRT chooses CUDA kernels to implement floating point operations in the network, it defaults to FP32 implementations. There are two ways to configure different levels of precision: To control precision at the model level, BuilderFlag options (C++, Python) can indicate to TensorRT that it may select lower-precision implementations when searching for the fastest (and because lower precision is generally faster, if allowed to, it typically will). Therefore, you can easily instruct TensorRT to use FP16 calculations for your entire model. For regularized models whose input dynamic range is approximately one, this typically produces significant speedups with negligible change in accuracy. For finer-grained control, where a layer must run at higher precision because part of the network is numerically sensitive or requires high dynamic range, arithmetic precision can be specified for that layer. Refer to the Reduced Precision section for more details."
TensorRT's Capabilities,Quantization,"TensorRT supports quantized floating point, where floating-point values are linearly compressed and rounded to 8-bit integers. This significantly increases arithmetic throughput while reducing storage requirements and memory bandwidth. When quantizing a floating-point tensor, TensorRT must know its dynamic range - that is, what range of values is important to represent - values outside this range are clamped when quantizing. Dynamic range information can be calculated by the builder (this is called calibration) based on representative input data. Or you can perform quantization-aware training in a framework and import the model to TensorRT with the necessary dynamic range information. Refer to the Working with INT8 chapter for more details."
TensorRT's Capabilities,Tensors and Data Formats,"When defining a network, TensorRT assumes that tensors are represented by multidimensional C-style arrays. Each layer has a specific interpretation of its inputs: for example, a 2D convolution will assume that the last three dimensions of its input are in CHW format - there is no option to use, for example a WHC format. Refer to the TensorRT Operator's Reference for how each layer interprets its inputs. Note that tensors are limited to at most 2^31-1 elements. While optimizing the network, TensorRT performs transformations internally (including to HWC, but also more complex formats) to use the fastest possible CUDA kernels. In general, formats are chosen to optimize performance, and applications have no control over the choices. However, the underlying data formats are exposed at I/O boundaries (network input and output, and passing data to and from plug-ins) to allow applications to minimize unnecessary format transformations. Refer to the I/O Formats section for more details."
TensorRT's Capabilities,Dynamic Shapes,"By default, TensorRT optimizes the model based on the input shapes (batch size, image size, and so on) at which it was defined. However, the builder can be configured to allow the input dimensions to be adjusted at runtime. In order to enable this, you specify one or more instances of OptimizationProfile (C++, Python) in the builder configuration, containing for each input a minimum and maximum shape, along with an optimization point within that range. TensorRT creates an optimized engine for each profile, choosing CUDA kernels that work for all shapes within the [minimum, maximum] range and are fastest for the optimization point - typically different kernels for each profile. You can then select among profiles at runtime. Refer to the Working with Dynamic Shapes chapter for more details."
TensorRT's Capabilities,DLA,"TensorRT supports NVIDIA’s Deep Learning Accelerator (DLA), a dedicated inference processor present on many NVIDIA SoCs that supports a subset of TensorRT’s layers. TensorRT allows you to execute part of the network on the DLA and the rest on GPU; for layers that can be executed on either device, you can select the target device in the builder configuration on a per-layer basis. Refer to the Working with DLA chapter for more details."
TensorRT's Capabilities,Updating Weights,"When building an engine, you can specify that it may later have its weights updated. This can be useful if you are frequently updating the weights of the model without changing the structure, such as in reinforcement learning or when retraining a model while retaining the same structure. Weight updates are performed using the Refitter (C++, Python) interface. Refer to the Refitting an Engine section for more details."
TensorRT's Capabilities,trtexec tool,Included in the samples directory is a command-line wrapper tool called trtexec. trtexec is a tool to use TensorRT without having to develop your own application. The trtexec tool has three main purposes: benchmarking networks on random or user-provided input data. generating serialized engines from models. generating a serialized timing cache from the builder. Refer to the trtexec section for more details.
TensorRT's Capabilities,Polygraphy,"Polygraphy is a toolkit designed to assist in running and debugging deep learning models in TensorRT and other frameworks. It includes a Python API and a command-line interface (CLI) built using this API. Among other things, with Polygraphy you can: Run inference among multiple backends, like TensorRT and ONNX-Runtime, and compare results (for example API,CLI). Convert models to various formats, for example, TensorRT engines with post-training quantization (for example API,CLI). View information about various types of models (for example CLI). Modify ONNX models on the command line: Extract subgraphs (for example CLI). Simplify and sanitize (for example CLI). Isolate faulty tactics in TensorRT (for example CLI). For more details, refer to the Polygraphy repository."
The C++ API,The Build Phase,"To create a builder, you first must instantiate the ILogger interface. You can then create an instance of the builder: IBuilder* builder = createInferBuilder(logger);"
The C++ API,Creating a Network Definition,"After the builder has been created, the first step in optimizing a model is to create a network definition: uint32_t flag = 1U <<static_cast<uint32_t> (NetworkDefinitionCreationFlag::kEXPLICIT_BATCH); INetworkDefinition* network = builder->createNetworkV2(flag); The kEXPLICIT_BATCH flag is required in order to import models using the ONNX parser. Refer to the Explicit Versus Implicit Batch section for more information."
The C++ API,Importing a Model Using the ONNX Parser,"Now, the network definition must be populated from the ONNX representation. The ONNX parser API is in the file NvOnnxParser.h, and the parser is in the nvonnxparser C++ namespace. #include “NvOnnxParser.h” using namespace nvonnxparser;"
The C++ API,Importing a Model Using the ONNX Parser,"You can create an ONNX parser to populate the network as follows: IParser*  parser = createParser(*network, logger);"
The C++ API,Importing a Model Using the ONNX Parser,"Then, read the model file and process any errors. parser->parseFromFile(modelFile,  static_cast<int32_t>(ILogger::Severity::kWARNING)); for (int32_t i = 0; i < parser.getNbErrors(); ++i) { std::cout << parser->getError(i)->desc() << std::endl; } An important aspect of a TensorRT network definition is that it contains pointers to model weights, which are copied into the optimized engine by the builder. Since the network was created using the parser, the parser owns the memory occupied by the weights, and so the parser object should not be deleted until after the builder has run."
The C++ API,Building an Engine,"The next step is to create a build configuration specifying how TensorRT should optimize the model. IBuilderConfig* config = builder->createBuilderConfig(); This interface has many properties that you can set in order to control how TensorRT optimizes the network. One important property is the maximum workspace size. Layer implementations often require a temporary workspace, and this parameter limits the maximum size that any layer in the network can use. If insufficient workspace is provided, it is possible that TensorRT will not be able to find an implementation for a layer. By default the workspace is set to the total global memory size of the given device; restrict it when necessary, for example, when multiple engines are to be built on a single device. config->setMemoryPoolLimit(MemoryPoolType::kWORKSPACE, 1U << 20); Once the configuration has been specified, the engine can be built. IHostMemory*  serializedModel = builder->buildSerializedNetwork(*network, *config); Since the serialized engine contains the necessary copies of the weights, the parser, network definition, builder configuration and builder are no longer necessary and may be safely deleted: delete parser; delete network; delete config; delete builder; The engine can then be saved to disk, and the buffer into which it was serialized can be deleted. delete serializedModel"
The C++ API,Building an Engine,"Note: Serialized engines are not portable across platforms or TensorRT versions. Engines are specific to the exact GPU model that they were built on (in addition to the platform and the TensorRT version). Since building engines is intended as an offline process, it can take significant time. Refer to the Optimizing Builder Performance section for how to make the builder run faster."
The C++ API,Deserializing a Plan,"Assuming you have previously serialized an optimized model and want to perform inference, you must create an instance of the Runtime interface. Like the builder, the runtime requires an instance of the logger: IRuntime* runtime = createInferRuntime(logger); After you have read the model into a buffer, you can deserialize it to obtain an engine: ICudaEngine* engine = runtime->deserializeCudaEngine(modelData, modelSize);"
The C++ API,Performing Inference,"The engine holds the optimized model, but to perform inference we must manage additional state for intermediate activations. This is done using the ExecutionContext interface: IExecutionContext *context = engine->createExecutionContext(); An engine can have multiple execution contexts, allowing one set of weights to be used for multiple overlapping inference tasks. (A current exception to this is when using dynamic shapes, when each optimization profile can only have one execution context.) To perform inference, you must pass TensorRT buffers for input and output, which TensorRT requires you to specify with calls to setTensorAddress, which takes the name of the tensor and the address of the buffer. You can query the engine using the names you provided for input and output tensors to find the right positions in the array: context->setTensorAddress(INPUT_NAME, inputBuffer); context->setTensorAddress(OUTPUT_NAME, outputBuffer); You can then call TensorRT’s method enqueueV3 to start inference asynchronously using a CUDA stream: context->enqueueV3(stream); It is common to enqueue data transfers with cudaMemcpyAsync() before and after the kernels to move data from the GPU if it is not already there. To determine when the kernels (and possibly cudaMemcpyAsyn()) are complete, use standard CUDA synchronization mechanisms such as events or waiting on the stream."
The Python API,Introduction,"This chapter illustrates basic usage of the Python API, assuming you are starting with an ONNX model. The onnx_resnet50.py sample illustrates this use case in more detail. The Python API can be accessed through the tensorrt module: import tensorrt as trt"
The Python API,The Build Phase,"To create a builder, you must first create a logger. The Python bindings include a simple logger implementation that logs all messages preceding a certain severity to stdout. logger = trt.Logger(trt.Logger.WARNING) Alternatively, it is possible to define your own implementation of the logger by deriving from the ILogger class: class MyLogger(trt.ILogger): def __init__(self): trt.ILogger.__init__(self) def log(self, severity, msg): pass # Your custom logging implementation here. logger = MyLogger() You can then create a builder: builder = trt.Builder(logger) Since building engines is intended as an offline process, it can take significant time. Refer to the Optimizing Builder Performance section for how to make the builder run faster."
The Python API,Creating a Network Definition,"After the builder has been created, the first step in optimizing a model is to create a network definition: network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) The EXPLICIT_BATCH flag is required in order to import models using the ONNX parser. Refer to the Explicit Versus Implicit Batch section for more information."
The Python API,Importing a Model Using the ONNX Parser,"Now, the network definition must be populated from the ONNX representation. You can create an ONNX parser to populate the network as follows: parser = trt.OnnxParser(network, logger) Then, read the model file and process any errors: success = parser.parse_from_file(model_path) for idx in range(parser.num_errors): print(parser.get_error(idx)) if not success: pass # Error handling code here"
The Python API,Building an Engine,"The next step is to create a build configuration specifying how TensorRT should optimize the model: config = builder.create_builder_config() This interface has many properties that you can set in order to control how TensorRT optimizes the network. One important property is the maximum workspace size. Layer implementations often require a temporary workspace, and this parameter limits the maximum size that any layer in the network can use. If insufficient workspace is provided, it is possible that TensorRT will not be able to find an implementation for a layer: config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 20) # 1 MiB After the configuration has been specified, the engine can be built and serialized with: serialized_engine = builder.build_serialized_network(network, config) It may be useful to save the engine to a file for future use. You can do that like so: with open(“sample.engine”, “wb”) as f: f.write(serialized_engine) Note: Serialized engines are not portable across platforms or TensorRT versions. Engines are specific to the exact GPU model that they were built on (in addition to the platform and the TensorRT version)."
The Python API,Deserializing a Plan,"To perform inference, deserialize the engine using the Runtime interface. Like the builder, the runtime requires an instance of the logger. runtime = trt.Runtime(logger) You can then deserialize the engine from a memory buffer: engine = runtime.deserialize_cuda_engine(serialized_engine) If you want, first load the engine from a file: with open(“sample.engine”, “rb”) as f: serialized_engine = f.read()"
The Python API,Performing Inference,"The engine holds the optimized model, but to perform inference requires additional state for intermediate activations. This is done using the IExecutionContext interface: context = engine.create_execution_context() An engine can have multiple execution contexts, allowing one set of weights to be used for multiple overlapping inference tasks. (A current exception to this is when using dynamic shapes, when each optimization profile can only have one execution context.) To perform inference, you must specify buffers for inputs and outputs: context.set_tensor_address(name, ptr) Several Python packages allow you to allocate memory on the GPU, including, but not limited to,the official CUDA Python bindings, PyTorch, cuPy, and Numba. After populating the input buffer, you can call TensorRT’s execute_async_v3 method to start inference asynchronously using a CUDA stream. First, create the CUDA stream. If you already have a CUDA stream, you can use a pointer to the existing stream. For example, for PyTorch CUDA streams, that is torch.cuda.Stream(), you can access the pointer using the cuda_stream property; for Polygraphy CUDA streams, use the ptr attribute. Next, start inference: context.execute_async_v3(buffers, stream_ptr) It is common to enqueue asynchronous transfers (cudaMemcpyAsync()) before and after the kernels to move data from the GPU if it is not already there. To determine when inference (and asynchronous transfers) are complete, use the standard CUDA synchronization mechanisms such as events or waiting on the stream. For example, with Polygraphy, use: stream.synchronize()"
How TensorRT Works,Object Lifetimes,"TensorRT’s API is class-based, with some classes acting as factories for other classes. For objects owned by the user, the lifetime of a factory object must span the lifetime of objects it creates. For example, the NetworkDefinition and BuilderConfig classes are created from the builder class, and objects of those classes should be destroyed before the builder factory object. An important exception to this rule is creating an engine from a builder. After you have created an engine, you may destroy the builder, network, parser, and build config and continue using the engine."
How TensorRT Works,Error Handling and Logging,"When creating TensorRT top-level interfaces (builder, runtime or refitter), you must provide an implementation of the Logger (C++, Python) interface. The logger is used for diagnostics and informational messages; its verbosity level is configurable. Since the logger may be used to pass back information at any point in the lifetime of TensorRT, its lifetime must span any use of that interface in your application. The implementation must also be thread-safe, since TensorRT may use worker threads internally. An API call to an object will use the logger associated with the corresponding top-level interface. For example, in a call to ExecutionContext::enqueueV3(), the execution context was created from an engine, which was created from a runtime, so TensorRT will use the logger associated with that runtime. The primary method of error handling is the ErrorRecorder (C++, Python) interface. You can implement this interface, and attach it to an API object to receive errors associated with that object. The recorder for an object will also be passed to any others it creates - for example, if you attach an error recorder to an engine, and create an execution context from that engine, it will use the same recorder. If you then attach a new error recorder to the execution context, it will receive only errors coming from that context. If an error is generated but no error recorder is found, it will be emitted through the associated logger. Note that CUDA errors are generally asynchronous - so when performing multiple inferences or other streams of CUDA work asynchronously in a single CUDA context, an asynchronous GPU error may be observed in a different execution context than the one that generated it."
How TensorRT Works,Memory,"TensorRT uses considerable amounts of device memory, that is, memory directly accessible by the GPU, as opposed to the host memory attached to the CPU). Since device memory is often a constrained resource, it is important to understand how TensorRT uses it."
How TensorRT Works,Memory - The Build Phase,"During build, TensorRT allocates device memory for timing layer implementations. Some implementations can consume a large amount of temporary memory, especially with large tensors. You can control the maximum amount of temporary memory through the builder’s maxWorkspace attribute. This defaults to the full size of device global memory but can be restricted when necessary. If the builder finds applicable kernels that could not be run because of insufficient workspace, it will emit a logging message indicating this. Even with relatively little workspace however, timing requires creating buffers for input, output, and weights. TensorRT is robust against the operating system (OS) returning out-of-memory for such allocations. On some platforms the OS may successfully provide memory, which then the out-of-memory killer process observes that the system is low on memory, and kills TensorRT. If this happens free up as much system memory as possible before retrying. During the build phase, there will typically be at least two copies of the weights in host memory: those from the original network, and those included as part of the engine as it is built. In addition, when TensorRT combines weights (for example convolution with batch normalization) additional temporary weight tensors will be created."
How TensorRT Works,Memory - The Runtime Phase,"At runtime, TensorRT uses relatively little host memory, but can use considerable amounts of device memory. An engine, on deserialization, allocates device memory to store the model weights. Since the serialized engine is almost all weights, its size is a good approximation to the amount of device memory the weights require. An ExecutionContext uses two kinds of device memory: Persistent memory required by some layer implementations - for example, some convolution implementations use edge masks, and this state cannot be shared between contexts as weights are, because its size depends on the layer input shape, which may vary across contexts. This memory is allocated on creation of the execution context, and lasts for its lifetime. Scratch memory, used to hold intermediate results while processing the network. This memory is used for intermediate activation tensors. It is also used for temporary storage required by layer implementations, the bound for which is controlled by IBuilderConfig::setMaxWorkspaceSize(). You may optionally create an execution context without scratch memory using ICudaEngine::createExecutionContextWithoutDeviceMemory() and provide that memory yourself for the duration of network execution. This allows you to share it between multiple contexts that are not running concurrently, or for other uses while inference is not running. The amount of scratch memory required is returned by ICudaEngine::getDeviceMemorySize(). Information about the amount of persistent memory and scratch memory used by the execution context is emitted by the builder when building the network, at severity kINFO. By default, TensorRT allocates device memory directly from CUDA. However, you can attach an implementation of TensorRT’s IGpuAllocator (C++, Python) interface to the builder or runtime and manage device memory yourself. This is useful if your application wants to control all GPU memory and suballocate to TensorRT instead of having TensorRT allocate directly from CUDA. TensorRT’s dependencies (cuDNN and cuBLAS) can occupy large amounts of device memory. TensorRT allows you to control whether these libraries are used for inference by using the TacticSources (C++, Python) attribute in the builder configuration. Note that some layer implementations require these libraries, so that when they are excluded, the network may not be compiled successfully. In addition, PreviewFeature::kDISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805 is used to control the usage of cuDNN, cuBLAS, and cuBLASLt in the TensorRT core library. When this flag is set, the TensorRT core library will not use these tactics even if they are specified by IBuilderConfig::setTacticSources(). This flag will not affect the cudnnContext and cublasContext handles passed to the plugins using IPluginV2Ext::attachToContext() if the appropriate tactic sources are set. The CUDA infrastructure and TensorRT’s device code also consume device memory. The amount of memory varies by platform, device, and TensorRT version. You can use cudaGetMemInfo to determine the total amount of device memory in use. TensorRT measures the amount of memory in use before and after critical operations in builder and runtime. These memory usage statistics are printed to TensorRT’s information logger. For example: [MemUsageChange] Init CUDA: CPU +535, GPU +0, now: CPU 547, GPU 1293 (MiB) It indicates the memory use change by CUDA initialization. CPU +535, GPU +0 is the increased amount of memory after running CUDA initialization. The content after now: is the CPU/GPU memory usage snapshot after CUDA initialization. Note: In a multi-tenant situation, the reported memory use by cudaGetMemInfo and TensorRT is prone to race conditions where a new allocation/free done by a different process or a different thread. Since CUDA is not in control of memory on unified-memory devices, the results returned by cudaGetMemInfo may not be accurate on these platforms."
How TensorRT Works,Memory - Lazy Module Loading,Lazy loading is a CUDA feature that can significantly reduce the peak GPU memory usage of TensorRT with negligible (< 1%) performance impact. The memory saved depends on the model and GPU platform. It is enabled by setting the environment variable CUDA_MODULE_LOADING=LAZY. Refer to the CUDA documentation for more information.
How TensorRT Works,Memory - L2 Persistent Cache Management,"NVIDIA Ampere and later architectures support L2 cache persistence, a feature which allows prioritization of L2 cache lines for retention when a line is chosen for eviction. TensorRT can use this to retain activations in cache, reducing DRAM traffic, and power consumption. Cache allocation is per-execution context, enabled using the context’s setPersistentCacheLimit method. The total persistent cache among all contexts (and other components using this feature) should not exceed cudaDeviceProp::persistingL2CacheMaxSize. Refer to the CUDA Best Practices Guide for more information."
How TensorRT Works,Threading,"In general, TensorRT objects are not thread safe; accesses to an object from different threads must be serialized by the client. The expected runtime concurrency model is that different threads will operate on different execution contexts. The context contains the state of the network (activation values, and so on) during execution, so using a context concurrently in different threads results in undefined behavior. To support this model, the following operations are thread safe: Nonmodifying operations on a runtime or engine. Deserializing an engine from a TensorRT runtime. Creating an execution context from an engine. Registering and deregistering plug-ins. There are no thread-safety issues with using multiple builders in different threads; however, the builder uses timing to determine the fastest kernel for the parameters provided, and using multiple builders with the same GPU will perturb the timing and TensorRT’s ability to construct optimal engines. There are no such issues using multiple threads to build with different GPUs."
How TensorRT Works,Determinism,"The TensorRT builder uses timing to find the fastest kernel to implement a given operator. Timing kernels is subject to noise - other work running on the GPU, fluctuations in GPU clock speed, and so on. Timing noise means that on successive runs of the builder, the same implementation may not be selected. In general, different implementations will use a different order of floating point operations, resulting in small differences in the output. The impact of these differences on the final result is usually very small. However, when TensorRT is configured to optimize by tuning over multiple precisions, the difference between an FP16 and an FP32 kernel can be more significant, particularly if the network has not been well regularized or is otherwise sensitive to numerical drift. Other configuration options that can result in a different kernel selection are different input sizes (for example, batch size) or a different optimization point for an input profile (refer to the Working with Dynamic Shapes section). The AlgorithmSelector (C++, Python) interface allows you to force the builder to pick a particular implementation for a given layer. You can use this to ensure that the same kernels are picked by the builder from run to run. For more information, refer to the Algorithm Selection and Reproducible Builds section. After an engine has been built, except for IFillLayer, it is deterministic: providing the same input in the same runtime environment will produce the same output."
How TensorRT Works,IFillLayer Determinism,"When IFillLayer is added to a network using either the RANDOM_UNIFORM or RANDOM_NORMAL operations, the determinism guarantee above is no longer valid. On each invocation, these operations generate tensors based on the RNG state, and then update the RNG state. This state is stored on a per-execution context basis."
